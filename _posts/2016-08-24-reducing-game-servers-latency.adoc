= 减小 Game Server 的网络延迟
:hp-alt-title: reducing game server's latency
:hp-tags: MMO, Server, Network

== 背景
之前 http://jx3.xoyo.com[剑网3] 游戏内的客户端到服务器的TCP网络延迟大约是 60~120ms。但是如果直接ping服务器的IP，会发现UDP包在物理网络上传输的实际延迟远远小于这个值，在比较好的网络环境下，甚至低于10ms。

稍加分析我们就可以看出，游戏内的实际网络延迟主要可以分为两大部分，一部分是协议包实际在网络上传输导致的延迟，另一部分则是服务器处理包所产生的延迟。

== 网络层面减小TCP的延迟
TCP本身是一个可靠的协议，理论上也只是在包头加上了额外的校验和序号信息。那么TCP是如何导致延迟增加的呢？其实，主要是TCP默认启用的 _Nagle's algorithm_。

_Nagle's algorithm_ 的目的是减少大量小包导致的Overhead过高的问题。源于早前Telnet的实现，客户端的每一个按键都会通过TCP发送，本身的信息也就数个字节，而TCP+IP的协议头最少都要40字节。于是在TCP协议栈的层面加入了合并小包的 _Nagle's algorithm_ 算法。算法的简单描述如下：
[source,c]
----
if there is new data to send
  if the window size >= MSS and available data is >= MSS
    send complete MSS segment now
  else
    if there is unconfirmed data still in the pipe
      enqueue data in the buffer until an acknowledge is received
    else
      send data immediately
    end if
  end if
end if
----
这里我们可以看到，_Nagle's algorithm_ 算法在拼包的时候会等ACK包，如果是一般的应用其实这样效率会比较高。但是对于游戏而言，本身的需求就不是最大化带宽利用率而是实时性。外加我们的游戏在应用层面本身就做了小包合并大包的算法，其实不需要TCP层面在做一次了，而由应用自己来决定什么时候需要等，什么时候应该发。实际的修改也比较简单。利用TCP的NO_DELAY参数关闭 _Nagle's algorithm_，然后应用层在一个网络处理循环内自己做拼包，在网络处理循环结束时Flush一下数据，这样保证数据在处理完之后就能被提交到系统层，这个时候如果物理层通畅，数据就会发出去，否则就在系统发送缓冲区等着。

== 应用层面提高网络处理的频率
MMO的游戏服务器通常都有一个帧的概念。就是游戏世界会按固定的帧速率运行。一来是为了公平，二来是游戏世界模拟的需要。通常从服务器负载的角度来考虑，这个帧数是不会太高了。通常在 10~30Hz 左右。理论上对于实时性不高的RPG游戏一帧处理一次网络请求然后返回结果给客户端就能满足需求了，服务器处理完当前帧就会开始Sleep，直到下一帧开始。但是这里有个问题，就是网络包可能正好错过了当前帧的Recv时机，从而需要等到下一帧才能被处理，这就导致对于这个网络包，其延时被额外加上了(1000/FPS)ms。如果服务器的FPS是10，那么这里就额外引入了100ms的延迟

随着整个游戏市场逐渐从RPG向打击感更爽快的ARPG转换。如何减小网络延迟对客户端手感的影响就变得比较重要。所以，服务器应该尽快的处理收到数据并适时得返回给客户端。也就是我们应该让网络处理的循环可以跑得尽可能快，而游戏世界模拟的帧数维持恒定。

这里我们讨论两种情况，一种是客户端向服务器发的移动指令。由于移动是和游戏世界模拟相关的，不能客户端发得快服务器上就跑得快，所以这种包，网络层收到后提交给游戏逻辑，游戏逻辑也就只是缓存一下状态，实际的位置移动，还是在游戏世界的模拟帧里面处理。另一种情况是客户端释放一个技能。由于不能相信客户端的数据，技能是否释放成功是由服务器判断的，所以整个施法流程是：客户端UI操作 -> 客户端逻辑预判并发送请求 -> 服务器判断并返回结果 -> 客户端完成施法并播放动作表现。也就是玩家按图标到看到画面反馈是有一个网络+服务器处理延迟的。这个时候服务器就应该处理尽快响应。当然技能会有CoolDown设计，所以并不会导致网速快就能多放技能的问题。

至于网络处理到底能多块，这里就有一个Sleep精度的问题了。因为不能让网络处理把CPU占满了，所以必须要Sleep。最早在Windows系统下，Sleep函数的精度大约是16ms，也就是60Hz左右。Linux下通常在10ms，100Hz。Windows NT内核可以通过API动态把这个精度提升到1ms，1000Hz。Linux并不能动态修改，而是内核编译时候的一个参数，不过好在CentOS 7的默认配置就是1000Hz。所以，理论上因为网络包的到达时机问题导致的延迟，可以低至1ms，远小于网络物理层的延迟，也就可以忽略不计了。
